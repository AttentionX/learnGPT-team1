# learnGPT - team 1

## week 1

### BPE tokenization?

- how does BPE work?
- explain how it works by referencing the A-shaped graph
- how is this better than character-level tokenization?

### `test_1.py`?

>  `GPTVer1` is a naive Bigram LM that performs poorly - why?

- "The quick brown fox jumps over the lazy:\nHAGdirdo sick's q-Whe,\n\nANs " - why gibberish at the end?
- In what ways bigarm LM's are limited by?

## week 2

...

## Contributors



